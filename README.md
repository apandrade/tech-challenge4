# POS FIAP ALURA - IA PARA DEVS
## Tech Challenge Fase 4
### Integrantes Grupo 26

- Andr√© Philipe Oliveira de Andrade(RM357002) - andrepoandrade@gmail.com
- Joir Neto (RM356391) - joirneto@gmail.com
- Marcos Jen San Hsie(RM357422) - marcosjsh@gmail.com
- Michael dos Santos Silva(RM357009) - michael.shel96@gmail.com
- Sonival dos Santos(RM356905) - sonival.santos@gmail.com

Video(Youtube): 

Github: https://github.com/apandrade/tech-challenge4

# An√°lise de V√≠deo com Detec√ß√£o de Pose e Emo√ß√µes

Este projeto realiza **an√°lise de v√≠deo** utilizando **Vis√£o Computacional e Aprendizado de M√°quina** para identificar poses humanas, detectar emo√ß√µes e relatar anomalias nos movimentos.  

## üõ†Ô∏è Funcionalidades

- **Detec√ß√£o de pose humana** usando **MediaPipe**  
- **Identifica√ß√£o de atividades** baseadas na posi√ß√£o do corpo  
- **Detec√ß√£o de emo√ß√µes faciais** utilizando **DeepFace**  
- **Registro de anomalias nos movimentos**  
- **Gera√ß√£o de relat√≥rio** com estat√≠sticas da an√°lise  
- **Processamento de v√≠deo** e salvamento de sa√≠da anotada  

---

## üöÄ Como Executar o Projeto

### 1Ô∏è‚É£ Clone do reposit√≥rio
```bash
git clone https://github.com/apandrade/tech-challenge4.git
cd tech-challenge4
```

### 2Ô∏è‚É£ Instale as depend√™ncias
```bash
pip install -r requirements.txt
```

### 3Ô∏è‚É£ Coloque um v√≠deo chamado `input_video.mp4` na mesma pasta do script e rode:
```bash
python analisando-video.py
```


## üìö Bibliotecas Utilizadas

O c√≥digo faz uso das seguintes bibliotecas:

| Biblioteca | Fun√ß√£o |
|------------|------------------------------------------------|
| `opencv-python` | Processamento de v√≠deo e exibi√ß√£o de sa√≠da |
| `mediapipe` | Detec√ß√£o da pose humana |
| `deepface` | Reconhecimento facial e an√°lise de emo√ß√µes |
| `tqdm` | Barra de progresso durante o processamento |
| `numpy` | C√°lculos matem√°ticos para an√°lise de poses |
| `os` | Manipula√ß√£o de diret√≥rios e arquivos |
| `time` | Medi√ß√£o de tempo de execu√ß√£o |

---

## üîç Estrutura do C√≥digo

### üèÉ Detec√ß√£o de Pose (MediaPipe)

- **`detect_pose_and_anomalies(frame, pose_model, last_pose_landmarks, frame_index, anomalies)`**  
  ‚Üí Processa cada frame para detectar poses e registra anomalias nos movimentos.
  
  Entrada: Frame, modelo de pose, landmarks do frame anterior, √≠ndice do frame e lista de anomalias.  
  Sa√≠da: Landmarks atuais.

- **`determine_activity(landmarks)`**  
  ‚Üí Analisa os pontos do corpo e classifica a atividade como "Em p√©", "Sentado", "Deitado", "Acenando", etc.
  
  Entrada: Landmarks detectados.  
  Sa√≠da: Atividade classificada (ex: caminhando, sentado, acenando).

- **`calculate_pose_difference(pose1, pose2)`**  
  ‚Üí Compara poses para identificar mudan√ßas bruscas que podem indicar anomalias.
  
  Entrada: Dois conjuntos de landmarks (pose1 e pose2).  
  Sa√≠da: Diferen√ßa m√©dia entre os landmarks.

- **`calculate_angle(a, b, c)`**
  ‚Üí Calcula o √¢ngulo entre tr√™s landmarks. Classifica movimentos com base nos √¢ngulos dos membros (bra√ßos, pernas).

  Entrada: Tr√™s landmarks (a, b, c).  
  Sa√≠da: √Çngulo em graus.

- **`is_hand_near_face(hand_landmark, nose_landmark, threshold)`**
  ‚Üí Verifica se a m√£o est√° pr√≥xima ao rosto. Detecta gestos como acenar ou co√ßar o rosto.

  Entrada: Landmark da m√£o, landmark do nariz e limiar de dist√¢ncia.  
  Sa√≠da: True se a m√£o estiver pr√≥xima ao rosto, False caso contr√°rio.

### üòÄ Detec√ß√£o de Emo√ß√µes (DeepFace)

- **`detect_emotions(frame)`**  
  ‚Üí Analisa express√µes faciais e retorna as emo√ß√µes detectadas usando o DeepFace. Identifica emo√ß√µes como felicidade, tristeza, raiva, etc.

  Entrada: Frame do v√≠deo.  
  Sa√≠da: Lista de emo√ß√µes detectadas.

- **`draw_emotions(frame, emotions)`**  
  ‚Üí Desenha caixas ao redor do rosto e exibe a emo√ß√£o detectada no frame.

  Entrada: Frame do v√≠deo e lista de emo√ß√µes.  
  Sa√≠da: Frame com as emo√ß√µes desenhadas.


### üé• Processamento de V√≠deo

- **`process_video(video_path, output_path, report_path)`**  
  ‚Üí Fun√ß√£o principal que executa todo o pipeline de processamento. L√™ o v√≠deo de entrada, analisa cada frame, salva um novo v√≠deo com as anota√ß√µes e gera um relat√≥rio.
  
  Entrada: Caminho do v√≠deo de entrada, caminho do v√≠deo de sa√≠da e caminho do relat√≥rio.  
  Sa√≠da: V√≠deo processado e relat√≥rio de an√°lise.

### üìä Relat√≥rio Gerado

Ap√≥s a execu√ß√£o, um **relat√≥rio de an√°lise** ser√° salvo em:

```
output/summary_report.txt
```

## üìå Exemplo de Sa√≠da

üîπ **V√≠deo anotado:**  
- Detec√ß√£o da pose com linhas e conex√µes desenhadas  
- Emo√ß√µes exibidas sobre os rostos detectados  
- Atividade classificada exibida no canto esquerdo do video  

üîπ **Relat√≥rio gerado:**  

```
Resumo da An√°lise do V√≠deo
Total de frames analisados: 3326
N√∫mero de anomalias detectadas: 1043
Emo√ß√µes detectadas:
Atividades detectadas
    -Escrevendo ou teclando: 386 vezes
    -Acenando: 23 vezes
    -Braco aberto: 119 vezes
    -Caminhando: 82 vezes
    -Deitado: 96 vezes
    -Em pe: 9 vezes
    -Mao no rosto: 235 vezes
    -Parado: 621 vezes
    -Sentado: 649 vezes
    -Atividade desconhecida: 493 vezes

Frames com anomalias:
5, 8, 9, 11, 12, 15, 16, 17, 21, 23, 30, 34, 38, 39, 40...
```

---

## üìù Observa√ß√µes

- O v√≠deo de entrada deve estar na pasta do script e ser nomeado `input_video.mp4`.  
- O modelo pode ter dificuldades em detectar poses corretamente se a ilumina√ß√£o estiver ruim ou houver muitas pessoas no v√≠deo.  

---

## ü§ñ Melhorias Futuras

- Melhorar detec√ß√£o e avalia√ß√£o. 
- Melhorar a precis√£o da **detec√ß√£o de anomalias** ajustando os **limiares de diferen√ßa**.  
- Implementar suporte para **detec√ß√£o de m√∫ltiplas pessoas** no mesmo v√≠deo.  
