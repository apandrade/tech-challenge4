# POS FIAP ALURA - IA PARA DEVS
## Tech Challenge Fase 4
### Integrantes Grupo 26

- Andr√© Philipe Oliveira de Andrade(RM357002) - andrepoandrade@gmail.com
- Joir Neto (RM356391) - joirneto@gmail.com
- Marcos Jen San Hsie(RM357422) - marcosjsh@gmail.com
- Michael dos Santos Silva(RM357009) - michael.shel96@gmail.com
- Sonival dos Santos(RM356905) - sonival.santos@gmail.com

Video(Youtube): https://youtu.be/fFAfOxIVIys?si=jWSriJLiemTKLL_B

Github: https://github.com/apandrade/tech-challenge4

# An√°lise de V√≠deo com Detec√ß√£o de Pose e Emo√ß√µes

Este projeto foi desenvolvido em Python e utiliza duas bibliotecas principais: MediaPipe, para detec√ß√£o de pose e landmarks corporais, e DeepFace, para an√°lise de emo√ß√µes. O objetivo √© analisar um v√≠deo de entrada, detectar movimentos e emo√ß√µes, e gerar um relat√≥rio com os resultados.  

O projeto processa v√≠deos frame a frame, detectando landmarks corporais (como ombros, cotovelos, pulsos, quadris, joelhos e tornozelos) e emo√ß√µes faciais. Com base nesses dados, o sistema classifica atividades e identifica anomalias, gerando um relat√≥rio final com os resultados.

## üõ†Ô∏è Funcionalidades

- **Detec√ß√£o de Pose:** Utiliza o MediaPipe para identificar landmarks corporais.  
- **Identifica√ß√£o de atividades:** Classifica movimentos com base na posi√ß√£o dos landmarks.  
- **Detec√ß√£o de emo√ß√µes faciais** utilizando **DeepFace**  
- **Registro de anomalias nos movimentos**  
- **Gera√ß√£o de relat√≥rio** Gera um relat√≥rio detalhado com anomalias detectadas, emo√ß√µes predominantes, atividades e an√¥malias.  
- **Processamento de v√≠deo** e salvamento de sa√≠da anotada  

---

## üöÄ Como Executar o Projeto

### 1Ô∏è‚É£ Clone do reposit√≥rio
```bash
git clone https://github.com/apandrade/tech-challenge4.git
cd tech-challenge4
```

### 2Ô∏è‚É£ Instale as depend√™ncias
```bash
pip install -r requirements.txt
```

### 3Ô∏è‚É£ Coloque um v√≠deo chamado `input_video.mp4` na mesma pasta do script e rode:
```bash
python analisando-video.py
```


## üìö Bibliotecas Utilizadas

O c√≥digo faz uso das seguintes bibliotecas:

| Biblioteca | Fun√ß√£o |
|------------|------------------------------------------------|
| `opencv-python` | Processamento de v√≠deo e exibi√ß√£o de sa√≠da |
| `mediapipe` | Detec√ß√£o da pose humana |
| `deepface` | Reconhecimento facial e an√°lise de emo√ß√µes |
| `tqdm` | Barra de progresso durante o processamento |
| `numpy` | C√°lculos matem√°ticos para an√°lise de poses |
| `os` | Manipula√ß√£o de diret√≥rios e arquivos |
| `time` | Medi√ß√£o de tempo de execu√ß√£o |

---

## üîç Estrutura do C√≥digo

### üèÉ Detec√ß√£o de Pose (MediaPipe)

- **`detect_pose_and_anomalies(frame, pose_model, last_pose_landmarks, frame_index, anomalies)`**  
  ‚Üí Processa cada frame para detectar poses e registra anomalias nos movimentos.
  
  Entrada: Frame, modelo de pose, landmarks do frame anterior, √≠ndice do frame e lista de anomalias.  
  Sa√≠da: Landmarks atuais.

- **`determine_activity(landmarks)`**  
  ‚Üí Analisa os pontos do corpo e classifica a atividade como "Em p√©", "Sentado", "Deitado", "Acenando", etc.
  
  Entrada: Landmarks detectados.  
  Sa√≠da: Atividade classificada (ex: caminhando, sentado, acenando).

- **`calculate_pose_difference(pose1, pose2)`**  
  ‚Üí Compara poses para identificar mudan√ßas bruscas que podem indicar anomalias.
  
  Entrada: Dois conjuntos de landmarks (pose1 e pose2).  
  Sa√≠da: Diferen√ßa m√©dia entre os landmarks.

- **`calculate_angle(a, b, c)`**
  ‚Üí Calcula o √¢ngulo entre tr√™s landmarks. Classifica movimentos com base nos √¢ngulos dos membros (bra√ßos, pernas).

  Entrada: Tr√™s landmarks (a, b, c).  
  Sa√≠da: √Çngulo em graus.

- **`is_hand_near_face(hand_landmark, nose_landmark, threshold)`**
  ‚Üí Verifica se a m√£o est√° pr√≥xima ao rosto. Detecta gestos como acenar ou co√ßar o rosto.

  Entrada: Landmark da m√£o, landmark do nariz e limiar de dist√¢ncia.  
  Sa√≠da: True se a m√£o estiver pr√≥xima ao rosto, False caso contr√°rio.

### üòÄ Detec√ß√£o de Emo√ß√µes (DeepFace)

- **`detect_emotions(frame)`**  
  ‚Üí Analisa express√µes faciais e retorna as emo√ß√µes detectadas usando o DeepFace. Identifica emo√ß√µes como felicidade, tristeza, raiva, etc.

  Entrada: Frame do v√≠deo.  
  Sa√≠da: Lista de emo√ß√µes detectadas.

- **`draw_emotions(frame, emotions)`**  
  ‚Üí Desenha caixas ao redor do rosto e exibe a emo√ß√£o detectada no frame.

  Entrada: Frame do v√≠deo e lista de emo√ß√µes.  
  Sa√≠da: Frame com as emo√ß√µes desenhadas.


### üé• Processamento de V√≠deo

- **`process_video(video_path, output_path, report_path)`**  
  ‚Üí Fun√ß√£o principal que executa todo o pipeline de processamento. O v√≠deo √© processado frame a frame. Para cada frame, os landmarks corporais s√£o detectados, as diferen√ßas de pose s√£o calculadas, e a atividade √© classificada. As emo√ß√µes detectadas s√£o desenhadas no frame, e as anomalias s√£o registradas para gerar um relat√≥rio final.
  
  Entrada: Caminho do v√≠deo de entrada, caminho do v√≠deo de sa√≠da e caminho do relat√≥rio.  
  Sa√≠da: V√≠deo processado e relat√≥rio de an√°lise.


## Destaques do Algoritmo
### Visibilidade dos Landmarks
Se um landmark n√£o estiver vis√≠vel, ele n√£o √© considerado na an√°lise. Por exemplo, se os tornozelos n√£o estiverem vis√≠veis, a pessoa provavelmente est√° sentada ou deitada.

### Dist√¢ncia entre os Landmarks
A dist√¢ncia entre os landmarks ajuda a inferir movimentos. Por exemplo, se o pulso est√° longe da cintura, √© prov√°vel que o bra√ßo esteja aberto.

### Observa√ß√£o das Posi√ß√µes
A posi√ß√£o relativa dos landmarks √© crucial. Se o pulso est√° acima do ombro, a pessoa pode estar acenando ou levantando a m√£o.

### Heur√≠stica dos √Çngulos
Os √¢ngulos entre os landmarks permitem classificar movimentos mais complexos. Por exemplo, um √¢ngulo grande entre o quadril, joelho e tornozelo indica que a pessoa est√° em p√© ou caminhando.


### üìä Relat√≥rio Gerado

Ap√≥s a execu√ß√£o, um **relat√≥rio de an√°lise** ser√° salvo em:

```
output/summary_report.txt
```

## üìå Exemplo de Sa√≠da

üîπ **V√≠deo anotado:**  
- Detec√ß√£o da pose com linhas e conex√µes desenhadas  
- Emo√ß√µes exibidas sobre os rostos detectados  
- Atividade classificada exibida no canto esquerdo do video  

üîπ **Relat√≥rio gerado:**  

```
Resumo da An√°lise do V√≠deo
Total de frames analisados: 3326
N√∫mero de anomalias detectadas: 1043
Emo√ß√µes detectadas:
	-sad: 874 vezes
	-fear: 417 vezes
	-happy: 1055 vezes
	-angry: 120 vezes
	-neutral: 834 vezes
	-surprise: 116 vezes
	-disgust: 1 vezes
Atividades detectadas
  -Escrevendo ou teclando: 386 vezes
  -Acenando: 23 vezes
  -Braco aberto: 119 vezes
  -Caminhando: 82 vezes
  -Deitado: 96 vezes
  -Em pe: 9 vezes
  -Mao no rosto: 235 vezes
  -Parado: 621 vezes
  -Sentado: 649 vezes
  -Atividade desconhecida: 493 vezes

Frames com anomalias:
5, 8, 9, 11, 12, 15, 16, 17, 21, 23, 30, 34, 38, 39, 40...
```

---

## üìù Observa√ß√µes

- O v√≠deo de entrada deve estar na pasta do script e ser nomeado `input_video.mp4`.  
- O modelo pode ter dificuldades em detectar poses corretamente se a ilumina√ß√£o estiver ruim ou houver muitas pessoas no v√≠deo.  

---

## üåü Considera√ß√µes Finais
Este projeto demonstra como t√©cnicas de vis√£o computacional podem ser usadas para analisar movimentos e emo√ß√µes humanas de forma automatizada.  
Futuramente, podemos expandir o projeto para incluir mais atividades, melhorar a precis√£o da detec√ß√£o e fazer integra√ß√µes com APIs externa para disponibilizar o servi√ßo na internet.

## ü§ñ Melhorias Futuras

- Melhorar detec√ß√£o e implementar novas atividades no algor√≠timo. 
- Melhorar a precis√£o da **detec√ß√£o de anomalias** ajustando os **limiares de diferen√ßa**.  
- Implementar suporte para **detec√ß√£o de m√∫ltiplas pessoas** no mesmo v√≠deo.
